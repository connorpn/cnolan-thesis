{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17364d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import io\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5fc04-2a28-45b3-915d-8305ec150cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912e7382",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"SET THE WORKING DIRECTORY BELOW TO THE LOCATION OF DATA FILES\"\n",
    "\n",
    "working_directory = 'C:/Users/conno/OneDrive/University Study/Honours Thesis/cnolan-thesis' #set location using back slashes\n",
    "\n",
    "os.chdir(working_directory)\n",
    "\n",
    "print(\"Current working directory: {0}\".format(os.getcwd()))\n",
    "\n",
    "\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            output_path = os.makedirs(directory)\n",
    "            print(output_path)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n",
    "        \n",
    "\n",
    "# Folder where outputs will be saved (by default a folder within the working directory) \n",
    "createFolder('./output/') \n",
    "output_path = working_directory +'./output/'\n",
    "\n",
    "print('Set WD: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e305ea",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NGER Greenhouse Gas and Energy Information by Corporation\n",
    "nger_2009 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2009.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2010 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2010.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2011 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2011.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2012 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2012.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2013 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2013.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2014 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2014.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2015 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2015.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2016 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2016.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2017 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2017.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2018 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2018.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2019 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2019.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2020 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2020.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2021 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2021.csv',encoding = \"ISO-8859-1\")\n",
    "\n",
    "#Morningstar Firm Data\n",
    "ms_capex = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/capex.csv', encoding='latin1')\n",
    "ms_eoy_price = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/eoy_price.csv', encoding='latin1')\n",
    "ms_eps = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/eps.csv', encoding='latin1')\n",
    "ms_ltdebt = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/longtermdebt.csv', encoding='latin1')\n",
    "ms_marketcap = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/marketcap.csv', encoding='latin1')\n",
    "ms_ppe = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/ppe.csv', encoding='latin1')\n",
    "ms_revenue = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/revenue.csv', encoding='latin1')\n",
    "ms_roe = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/roe.csv', encoding='latin1')\n",
    "ms_stdebt = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/shorttermdebt.csv', encoding='latin1')\n",
    "ms_assets = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/totalassets.csv', encoding='latin1')\n",
    "ms_liabilities = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/totalliabilities.csv', encoding='latin1')\n",
    "\n",
    "#Datastream Data (data is of nger-morningstar matched firms only)\n",
    "ds_price_data = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/ds_matched_dailyprice.csv') #daily price data of matched nger firms\n",
    "ds_beta = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/ds_beta.csv')\n",
    "ds_marketcap = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/ds_marketcap.csv')\n",
    "ds_industry = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/ds_ticker_industry.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837252f",
   "metadata": {},
   "source": [
    "# Clean Data: NGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01027a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Keep only Firm Name, Scope 1 Emissions, Scope 2 Emissions, and Energy Consumption Columns\"\n",
    "nger_2012 = nger_2012.drop(nger_2012.columns[[3]],axis=1)\n",
    "nger_2013 = nger_2013.drop(nger_2013.columns[[4]],axis=1)\n",
    "nger_2014 = nger_2014.drop(nger_2014.columns[[1,5]],axis=1)\n",
    "nger_2015 = nger_2015.drop(nger_2015.columns[[1,5]],axis=1)\n",
    "nger_2016 = nger_2016.drop(nger_2016.columns[[1,5]],axis=1)\n",
    "nger_2017 = nger_2017.drop(nger_2017.columns[[1,5]],axis=1)\n",
    "nger_2018 = nger_2018.drop(nger_2018.columns[[1,5]],axis=1)\n",
    "nger_2019 = nger_2019.drop(nger_2019.columns[[1,5]],axis=1)\n",
    "nger_2020 = nger_2020.drop(nger_2020.columns[[1,5]],axis=1)\n",
    "nger_2021 = nger_2021.drop(nger_2021.columns[[1,5]],axis=1)\n",
    "\n",
    "\n",
    "\"Standarize Headings\"\n",
    "\n",
    "nger_list = [nger_2009,nger_2010,nger_2011,nger_2012,nger_2013,nger_2014,nger_2015,nger_2016,nger_2017,nger_2018,nger_2019,nger_2020,nger_2021]\n",
    "\n",
    "def standard_names(dataset):\n",
    "    \n",
    "    dataset.columns = ['firm_name', 'scope1', 'scope2','energy_consumption']\n",
    "\n",
    "for i in nger_list:\n",
    "    standard_names(i)\n",
    "\n",
    "\n",
    "\"Add Year To Datasets\"\n",
    "#add in year\n",
    "def add_year(dataset,year):\n",
    "    dataset.insert(0, 'year', year)\n",
    "#Registered Corporation Datasets\n",
    "add_year(nger_2009,2009)\n",
    "add_year(nger_2010,2010)\n",
    "add_year(nger_2011,2011)\n",
    "add_year(nger_2012,2012)\n",
    "add_year(nger_2013,2013)\n",
    "add_year(nger_2014,2014)\n",
    "add_year(nger_2015,2015)\n",
    "add_year(nger_2016,2016)\n",
    "add_year(nger_2017,2017)\n",
    "add_year(nger_2018,2018)\n",
    "add_year(nger_2019,2019)\n",
    "add_year(nger_2020,2020)\n",
    "add_year(nger_2021,2021)\n",
    "\n",
    "\n",
    "\"Construct Total Emissions\"\n",
    "\n",
    "def calculate_total_emissions(dataset):\n",
    "    dataset['total_emissions'] = dataset['scope1'] + dataset['scope2']\n",
    "    \n",
    "\n",
    "nger_list = [nger_2009,nger_2010,nger_2011,nger_2012,nger_2013,nger_2014,nger_2015,nger_2016,nger_2017,nger_2018,nger_2019,nger_2020,nger_2021]\n",
    "\n",
    "for i in nger_list:\n",
    "    calculate_total_emissions(i)\n",
    "\n",
    "\n",
    "\"Combined Datasets\"\n",
    "nger_data = pd.DataFrame(columns=['year', 'corporation', 'scope1', 'scope2', 'energy_consumption','total_emissions'])\n",
    "nger_list = [nger_2009,nger_2010,nger_2011,nger_2012,nger_2013,nger_2014,nger_2015,nger_2016,nger_2017,nger_2018,nger_2019,nger_2020,nger_2021]\n",
    "nger_data = pd.concat(nger_list)\n",
    "print(list(nger_data))\n",
    "\n",
    "\n",
    "\n",
    "\"Clean up nger_data\"\n",
    "\n",
    "#remove comma's from emissions values \n",
    "nger_data['scope1'] = nger_data['scope1'].replace(\",\", \"\", regex=True)\n",
    "nger_data['scope2'] = nger_data['scope2'].replace(\",\", \"\", regex=True)\n",
    "nger_data['total_emissions'] = nger_data['total_emissions'].replace(\",\", \"\", regex=True)\n",
    "nger_data['energy_consumption'] = nger_data['energy_consumption'].replace(\",\", \"\", regex=True)\n",
    "\n",
    "#remove spaces's from emissions values \n",
    "nger_data['scope1'] = nger_data['scope1'].replace(\" \", \"\", regex=True)\n",
    "nger_data['scope2'] = nger_data['scope2'].replace(\" \", \"\", regex=True)\n",
    "nger_data['total_emissions'] = nger_data['total_emissions'].replace(\" \", \"\", regex=True)\n",
    "nger_data['energy_consumption'] = nger_data['energy_consumption'].replace(\" \", \"\", regex=True)\n",
    "\n",
    "#remove alphanumeric charaters from emissions values\n",
    "nger_data['scope1'] = nger_data['scope1'].str.replace('[a-z]', '', regex=True, flags=re.IGNORECASE)\n",
    "nger_data['scope2'] = nger_data['scope2'].str.replace('[a-z]', '',regex=True, flags=re.IGNORECASE)\n",
    "nger_data['total_emissions'] = nger_data['total_emissions'].str.replace('[a-z]', '',regex=True, flags=re.IGNORECASE)\n",
    "nger_data['energy_consumption'] = nger_data['energy_consumption'].str.replace('[a-z]', '',regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "#replace blanks with nan\n",
    "nger_data = nger_data.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "#drop rows with missing scope1 & 2 data\n",
    "nger_data = nger_data.dropna(axis=0, how= 'all', subset=['scope1', 'scope2'])\n",
    "\n",
    "#convert emissions values to float\n",
    "nger_data['scope1'] = nger_data['scope1'].astype(float)\n",
    "nger_data['scope2'] = nger_data['scope2'].astype(float)\n",
    "nger_data['total_emissions'] = nger_data['total_emissions'].astype(float)\n",
    "nger_data['energy_consumption'] = nger_data['energy_consumption'].astype(float)\n",
    "\n",
    "#convert year column to date time format\n",
    "#nger_data['year'] =  pd.to_datetime(nger_data['year'], format='%Y').dt.to_period(\"Y\")\n",
    "nger_data['year'] = nger_data['year'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "\"FORMAT NGER COMPANY NAMES FOR MATCHING\"\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.lower()\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('pty ltd','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('limited group','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('limited','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('group','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('ltd','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('holdings','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('holding','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('pty','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('proprietary','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('corporation','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.rstrip('.')\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.strip()\n",
    "nger_data = nger_data.rename(columns={'firm_name':'nger_name'})\n",
    "\n",
    "nger_data = nger_data.reset_index(drop=True)\n",
    "\n",
    "\"Extract unique nger firms\"\n",
    "nger_firms_list = []\n",
    "nger_firms_list = nger_data['nger_name'].unique().tolist()\n",
    "\n",
    "\n",
    "nger_data = nger_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\"Save to csv file\"\n",
    "output_filename = 'nger_data.csv'\n",
    "outputname = output_path + output_filename\n",
    "nger_data.to_csv(outputname, mode='w')\n",
    "print ('Exported: '+outputname)\n",
    "\n",
    "print('Number of Observations:')\n",
    "print (len(nger_data))\n",
    "\n",
    "\n",
    "with pd.option_context('display.max_rows', 100, 'display.max_columns', 100):\n",
    "    display(nger_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3cd3b1",
   "metadata": {},
   "source": [
    "# Clean Data: Morningstar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Melt Data to Time Series Format\"\n",
    "ms_capex = pd.melt(ms_capex, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='capex',col_level=None)\n",
    "ms_eoy_price = pd.melt(ms_eoy_price, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='eoy_price',col_level=None)\n",
    "ms_eps = pd.melt(ms_eps, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='eps',col_level=None)\n",
    "ms_ltdebt = pd.melt(ms_ltdebt, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='ltdebt',col_level=None)\n",
    "ms_marketcap = pd.melt(ms_marketcap, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='marketcap',col_level=None)\n",
    "ms_ppe = pd.melt(ms_ppe, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='ppe',col_level=None)\n",
    "ms_revenue = pd.melt(ms_revenue, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='revenue',col_level=None)\n",
    "ms_roe = pd.melt(ms_roe, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='roe',col_level=None)\n",
    "ms_stdebt = pd.melt(ms_stdebt, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='stdebt',col_level=None)\n",
    "ms_assets = pd.melt(ms_assets, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='assets',col_level=None)\n",
    "ms_liabilities = pd.melt(ms_liabilities, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='liabilities',col_level=None)\n",
    "\n",
    "\"Merge Data Sets\"\n",
    "\n",
    "#make capex the main dataset to merge on\n",
    "ms_data = ms_capex\n",
    "#merge eoy_price\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex']], ms_eoy_price[['ASX Code', 'year','eoy_price']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge eps\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price']], ms_eps[['ASX Code', 'year','eps']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge ltdebt\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price','eps']], ms_ltdebt[['ASX Code', 'year','ltdebt']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge marketcap\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price','eps','ltdebt']], ms_marketcap[['ASX Code', 'year','marketcap']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge ppe\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price','eps','ltdebt','marketcap']],  ms_ppe[['ASX Code', 'year','ppe']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge revenue\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price','eps','ltdebt','marketcap','ppe']], ms_revenue[['ASX Code', 'year','revenue']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge roe\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price','eps','ltdebt','marketcap','ppe','revenue']], ms_roe[['ASX Code', 'year','roe']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge stdebt\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price','eps','ltdebt','marketcap','ppe','revenue','roe']], ms_stdebt[['ASX Code', 'year','stdebt']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge assets\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price','eps','ltdebt','marketcap','ppe','revenue','roe','stdebt']], ms_assets[['ASX Code', 'year','assets']],on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge liabilities\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price','eps','ltdebt','marketcap','ppe','revenue','roe','stdebt','assets']], ms_liabilities[['ASX Code', 'year','liabilities']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "\n",
    "\n",
    "\"Clean up ['capex']\"\n",
    "#remove comma's from capex column\n",
    "ms_data[\"capex\"] = ms_data[\"capex\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"capex\"] = ms_data[\"capex\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"capex\"] = ms_data[\"capex\"].replace('--', np.nan, regex=True)\n",
    "#convert capex values to float\n",
    "ms_data['capex'] = ms_data['capex'].astype(float)\n",
    "\n",
    "\"Clean up ['eoy_price']\"\n",
    "#remove comma's from eoy_price values \n",
    "ms_data[\"eoy_price\"] = ms_data[\"eoy_price\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"eoy_price\"] = ms_data[\"eoy_price\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"eoy_price\"] = ms_data[\"eoy_price\"].replace('--', np.nan, regex=True)\n",
    "#convert eoy_price values to float\n",
    "ms_data['eoy_price'] =ms_data['eoy_price'].astype(float)\n",
    "#replace negative numbers with nan\n",
    "ms_data.loc[ms_data['eoy_price'] < 0, 'eoy_price'] = np.nan\n",
    "#replace 0 with nan\n",
    "ms_data['eoy_price'] = ms_data['eoy_price'].replace(0, np.nan)\n",
    "\n",
    "\n",
    "\"Clean up ['eps']\"\n",
    "#remove comma's from revenue values \n",
    "ms_data[\"eps\"] = ms_data[\"eps\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"eps\"] = ms_data[\"eps\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"eps\"] = ms_data[\"eps\"].replace('--', np.nan, regex=True)\n",
    "#convert eps values to float\n",
    "ms_data['eps'] = ms_data[\"eps\"].astype(float)\n",
    "\n",
    "\"Clean up ['ltdebt']\"\n",
    "#remove comma's from ltdebt  \n",
    "ms_data[\"ltdebt\"] = ms_data[\"ltdebt\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"ltdebt\"] = ms_data[\"ltdebt\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"ltdebt\"] = ms_data[\"ltdebt\"].replace('--', np.nan, regex=True)\n",
    "#convert ltdebt values to float\n",
    "ms_data['ltdebt'] = ms_data['ltdebt'].astype(float)\n",
    "#replace 0 with nan\n",
    "ms_data['ltdebt'] = ms_data['ltdebt'].replace(0, np.nan)\n",
    "#replace negative numbers with nan\n",
    "ms_data.loc[ms_data['ltdebt'] < 0, 'ltdebt'] = np.nan\n",
    "\n",
    "\"Clean up ['marketcap']\"\n",
    "#replace blanks with nan\n",
    "ms_data['marketcap'] = ms_data['marketcap'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#convert marketcap values to float\n",
    "ms_data['marketcap'] = ms_data['marketcap'].astype(float)\n",
    "#replace negative numbers with nan\n",
    "ms_data.loc[ms_data['marketcap'] < 0, 'marketcap'] = np.nan\n",
    "#replace 0 with nan\n",
    "ms_data['marketcap'] = ms_data['marketcap'].replace(0, np.nan)\n",
    "\n",
    "\"Clean up ['ppe']\"\n",
    "\n",
    "#remove comma's from ppe value \n",
    "ms_data[\"ppe\"] = ms_data[\"ppe\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"ppe\"] = ms_data[\"ppe\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"ppe\"] = ms_data[\"ppe\"].replace('--', np.nan, regex=True)\n",
    "#convert ppe values to float\n",
    "ms_data['ppe'] = ms_data[\"ppe\"].astype(float)\n",
    "#replace negative numbers with nan\n",
    "ms_data.loc[ms_data['ppe'] < 0, 'ppe'] = np.nan\n",
    "\n",
    "\"Clean up ['revenue']\"\n",
    "#remove comma's from revenue values \n",
    "ms_data[\"revenue\"] = ms_data[\"revenue\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"revenue\"] = ms_data[\"revenue\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"revenue\"] = ms_data[\"revenue\"].replace('--', np.nan, regex=True)\n",
    "#convert revenue values to float\n",
    "ms_data['revenue'] = ms_data[\"revenue\"].astype(float)\n",
    "#replace negative numbers with nan\n",
    "ms_data.loc[ms_data['revenue'] < 0, 'revenue'] = np.nan\n",
    "#replace 0 with nan\n",
    "ms_data['revenue'] = ms_data['revenue'].replace(0, np.nan)\n",
    "\n",
    "\"Clean up ['roe']\"\n",
    "#remove comma's from roe \n",
    "ms_data[\"roe\"] = ms_data[\"roe\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"roe\"] = ms_data[\"roe\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"roe\"] = ms_data[\"roe\"].replace('--', np.nan, regex=True)\n",
    "#convert percentage string to float for roe \n",
    "ms_data['roe'] = (pd.to_numeric(ms_data['roe'].str[:-1]).div(100).mask(ms_data['roe']=='%',0))\n",
    "\n",
    "\"Clean up ['stdebt']\"\n",
    "#remove comma's from stdebt  \n",
    "ms_data[\"stdebt\"] = ms_data[\"stdebt\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"stdebt\"] = ms_data[\"stdebt\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"stdebt\"] = ms_data[\"stdebt\"].replace('--', np.nan, regex=True)\n",
    "#convert stdebt values to float\n",
    "ms_data['stdebt'] = ms_data[\"stdebt\"].astype(float)\n",
    "#replace negative numbers with nan\n",
    "ms_data.loc[ms_data['stdebt'] < 0, 'stdebt'] = np.nan\n",
    "#replace 0 with nan\n",
    "ms_data['stdebt'] = ms_data['stdebt'].replace(0, np.nan)\n",
    "\n",
    "\"Clean up ['assets']\"\n",
    "#remove comma's from total asset value \n",
    "ms_data[\"assets\"] = ms_data[\"assets\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"assets\"] = ms_data[\"assets\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"assets\"] = ms_data[\"assets\"].replace('--', np.nan, regex=True)\n",
    "#convert marketcap values to float\n",
    "ms_data[\"assets\"] = ms_data[\"assets\"].astype(float)\n",
    "#replace negative numbers with nan\n",
    "ms_data.loc[ms_data['assets'] < 0, 'assets'] = np.nan\n",
    "#replace 0 with nan\n",
    "ms_data['assets'] = ms_data['assets'].replace(0, np.nan)\n",
    "\n",
    "\"Clean up ['liabilities']\"\n",
    "#remove comma's from total asset value \n",
    "ms_data[\"liabilities\"] = ms_data[\"liabilities\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"liabilities\"] = ms_data[\"liabilities\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"liabilities\"] = ms_data[\"liabilities\"].replace('--', np.nan, regex=True)\n",
    "#convert marketcap values to float\n",
    "ms_data['liabilities'] = ms_data[\"liabilities\"].astype(float)\n",
    "#replace negative numbers with nan\n",
    "ms_data.loc[ms_data['liabilities'] < 0, 'liabilities'] = np.nan\n",
    "\"Convert year column to date time format\"\n",
    "ms_data['year'] =  pd.to_datetime(ms_data['year'], format='%Y').dt.to_period(\"Y\")\n",
    "\n",
    "\"FORMAT MORNINGSTAR COMPANY NAMES FOR MATCHING\"\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.lower()\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('pty ltd','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('limited group','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('limited','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('group','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('ltd.','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('ltd','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('holdings','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('holding','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('pty.','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('proprietary','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('corporation','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.rstrip('.')\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.strip()\n",
    "ms_data = ms_data.rename(columns={'Company Name':'morningstar_name','ASX Code':'ticker'})\n",
    "\n",
    "\n",
    "\"Save collated file\"\n",
    "output_filename = 'ms_data.csv'\n",
    "outputname = output_path + output_filename\n",
    "ms_data.to_csv(outputname, mode='w', index=False)\n",
    "print(\"Exported File: \" + outputname)\n",
    "\n",
    "print('Number of Observations:')\n",
    "print (len(ms_data))\n",
    "\n",
    "\n",
    "with pd.option_context('display.max_rows', 100, 'display.max_columns', None):\n",
    "    display(ms_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929bd9bd-44e7-4cd2-8f7b-68e245e7a94d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fama French Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cb0516-cfc5-410f-9880-c0d5af2501f2",
   "metadata": {},
   "source": [
    "Fama French Factors (asia pacific ex japn) (From K.R.French Websbite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cbc4dc-7685-4d36-883d-f56c1afec145",
   "metadata": {},
   "source": [
    "## Collate Fama French Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782b1453-3846-416e-bd74-6451d7b7a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "fama_french_5_factors = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/Asia_Pacific_ex_Japan_5_Factors.csv')\n",
    "fama_french_mom_factor = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/Asia_Pacific_ex_Japan_MOM_Factor.csv')\n",
    "print(list(fama_french_5_factors))\n",
    "print(list(fama_french_mom_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b5219-3ca8-4649-be74-ec539ea0d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "fama_french_factors = pd.merge(fama_french_5_factors, fama_french_mom_factor[['yearmonth', 'wml']], on = ['yearmonth'], how = 'outer')\n",
    "fama_french_factors = fama_french_factors[fama_french_factors['yearmonth'] > 200805] \n",
    "fama_french_factors = fama_french_factors[fama_french_factors['yearmonth'] < 202106]  \n",
    "print(list(fama_french_factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5953da2e-6766-4e3c-b41b-a6587874a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_before = len(fama_french_factors)\n",
    "fama_french_factors = fama_french_factors.dropna(subset=['mktrf', 'smb', 'hml', 'rmw', 'cma','wml'], how='any').reset_index(drop=True)\n",
    "obs_after = len (fama_french_factors)\n",
    "obs_dropped = obs_before - obs_after\n",
    "print('dropna(variables): ' + str(obs_dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f834dd1-17f3-4cd9-9037-f589a6fe8b4a",
   "metadata": {},
   "source": [
    "## Convert Fama French Factors to AUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b145641-0a13-433b-bc60-9f46ac9595a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "usdaud_exchange_rate = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/usdaud_exchange_rate.csv')\n",
    "usdaud_exchange_rate = usdaud_exchange_rate[usdaud_exchange_rate['yearmonth'] > 200805] \n",
    "usdaud_exchange_rate = usdaud_exchange_rate[usdaud_exchange_rate['yearmonth'] < 202106]\n",
    "print(list(usdaud_exchange_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e3ab1e-9b39-412f-b328-1e01fbd6e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "fama_french_factors = pd.merge(fama_french_factors, usdaud_exchange_rate [['yearmonth','usd_to_aud']], on=['yearmonth'],how='left')\n",
    "print(list(fama_french_factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d9dae-8399-487d-a66c-429d3afb6641",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_before = len(fama_french_factors)\n",
    "fama_french_factors = fama_french_factors.dropna(subset=['mktrf', 'smb', 'hml', 'rmw', 'cma','wml', 'usd_to_aud'], how='any').reset_index(drop=True)\n",
    "obs_after = len (fama_french_factors)\n",
    "obs_dropped = obs_before - obs_after\n",
    "print('dropna(variables): ' + str(obs_dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6163030-ef38-41cf-ad49-5db4b1a91f3c",
   "metadata": {},
   "source": [
    "## Export Fama French Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53a158-ab4d-4cb6-911c-bc12713ba183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Save\"\n",
    "output_filename = 'famafrench.csv'\n",
    "outputname = output_path + output_filename\n",
    "fama_french_factors.to_csv(outputname, mode='w', index=False)\n",
    "print(\"Exported File: \" + outputname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7716c1-74fa-4a1d-8019-16ac471e661f",
   "metadata": {},
   "source": [
    "# Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb59c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Notebook Finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
