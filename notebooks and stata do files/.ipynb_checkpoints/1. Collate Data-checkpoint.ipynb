{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c34b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import io\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f10107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\conno\\OneDrive\\University Study\\Honours Thesis\\cnolan-thesis\n",
      "Set WD: Done\n"
     ]
    }
   ],
   "source": [
    "\"SET THE WORKING DIRECTORY BELOW TO THE LOCATION OF DATA FILES\"\n",
    "\n",
    "working_directory = 'C:/Users/conno/OneDrive/University Study/Honours Thesis/cnolan-thesis' #set location using back slashes\n",
    "\n",
    "os.chdir(working_directory)\n",
    "\n",
    "print(\"Current working directory: {0}\".format(os.getcwd()))\n",
    "\n",
    "\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            output_path = os.makedirs(directory)\n",
    "            print(output_path)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n",
    "        \n",
    "\n",
    "# Folder where outputs will be saved (by default a folder within the working directory) \n",
    "createFolder('./output/') \n",
    "output_path = working_directory +'./output/'\n",
    "\n",
    "print('Set WD: Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41085008",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0f1368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NGER Greenhouse Gas and Energy Information by Corporation\n",
    "nger_2009 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2009.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2010 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2010.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2011 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2011.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2012 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2012.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2013 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2013.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2014 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2014.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2015 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2015.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2016 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2016.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2017 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2017.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2018 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2018.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2019 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2019.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2020 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2020.csv',encoding = \"ISO-8859-1\")\n",
    "nger_2021 = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/NGER_GHGANDENERGYINFO_RC_2021.csv',encoding = \"ISO-8859-1\")\n",
    "\n",
    "#Morningstar Firm Data\n",
    "ms_capex = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/capex.csv', encoding='latin1')\n",
    "ms_eoy_price = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/eoy_price.csv', encoding='latin1')\n",
    "ms_eps = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/eps.csv', encoding='latin1')\n",
    "ms_ltdebt = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/longtermdebt.csv', encoding='latin1')\n",
    "ms_marketcap = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/marketcap.csv', encoding='latin1')\n",
    "ms_ppe = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/ppe.csv', encoding='latin1')\n",
    "ms_revenue = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/revenue.csv', encoding='latin1')\n",
    "ms_roe = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/roe.csv', encoding='latin1')\n",
    "ms_stdebt = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/shorttermdebt.csv', encoding='latin1')\n",
    "ms_assets = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/totalassets.csv', encoding='latin1')\n",
    "ms_liabilities = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/totalliabilities.csv', encoding='latin1')\n",
    "\n",
    "#Datastream Data (data is of nger-morningstar matched firms only)\n",
    "ds_price_data = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/ds_matched_dailyprice.csv') #daily price data of matched nger firms\n",
    "ds_beta = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/ds_beta.csv')\n",
    "ds_marketcap = pd.read_csv('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/ds_marketcap.csv')\n",
    "ds_industry = pd.read_csv ('https://raw.githubusercontent.com/connorpn/cnolan-thesis/main/data/ds_ticker_industry.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572050a4",
   "metadata": {},
   "source": [
    "# Clean Data: NGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c3831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['year', 'firm_name', 'scope1', 'scope2', 'energy_consumption', 'total_emissions']\n",
      "Exported: C:/Users/conno/OneDrive/University Study/Honours Thesis/cnolan-thesis./output/nger_data.csv\n",
      "Number of Observations:\n",
      "5055\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>nger_name</th>\n",
       "      <th>scope1</th>\n",
       "      <th>scope2</th>\n",
       "      <th>energy_consumption</th>\n",
       "      <th>total_emissions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009</td>\n",
       "      <td>a j bush &amp; sons</td>\n",
       "      <td>122169.0</td>\n",
       "      <td>18750.0</td>\n",
       "      <td>1083428.0</td>\n",
       "      <td>1.221692e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009</td>\n",
       "      <td>a.a. scott</td>\n",
       "      <td>120151.0</td>\n",
       "      <td>7649.0</td>\n",
       "      <td>1754221.0</td>\n",
       "      <td>1.201518e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009</td>\n",
       "      <td>aapc</td>\n",
       "      <td>16612.0</td>\n",
       "      <td>179939.0</td>\n",
       "      <td>1006315.0</td>\n",
       "      <td>1.661218e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009</td>\n",
       "      <td>abb grain</td>\n",
       "      <td>89433.0</td>\n",
       "      <td>72922.0</td>\n",
       "      <td>1889997.0</td>\n",
       "      <td>8.943373e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009</td>\n",
       "      <td>actew</td>\n",
       "      <td>55149.0</td>\n",
       "      <td>165307.0</td>\n",
       "      <td>2715397.0</td>\n",
       "      <td>5.514917e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5050</th>\n",
       "      <td>2021</td>\n",
       "      <td>digital australia investment management</td>\n",
       "      <td>149.0</td>\n",
       "      <td>59706.0</td>\n",
       "      <td>249821.0</td>\n",
       "      <td>1.495971e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5051</th>\n",
       "      <td>2021</td>\n",
       "      <td>mcdonald's australia</td>\n",
       "      <td>138.0</td>\n",
       "      <td>74021.0</td>\n",
       "      <td>331386.0</td>\n",
       "      <td>1.387402e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5052</th>\n",
       "      <td>2021</td>\n",
       "      <td>global switch australia</td>\n",
       "      <td>85.0</td>\n",
       "      <td>129503.0</td>\n",
       "      <td>576792.0</td>\n",
       "      <td>8.512950e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5053</th>\n",
       "      <td>2021</td>\n",
       "      <td>moorabool wind farm ()</td>\n",
       "      <td>64.0</td>\n",
       "      <td>68044.0</td>\n",
       "      <td>250872.0</td>\n",
       "      <td>6.468044e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5054</th>\n",
       "      <td>2021</td>\n",
       "      <td>lyondellbasell australia ()</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56327.0</td>\n",
       "      <td>58250.0</td>\n",
       "      <td>2.563270e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5055 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year                                nger_name    scope1    scope2  \\\n",
       "0     2009                          a j bush & sons  122169.0   18750.0   \n",
       "1     2009                               a.a. scott  120151.0    7649.0   \n",
       "2     2009                                     aapc   16612.0  179939.0   \n",
       "3     2009                                abb grain   89433.0   72922.0   \n",
       "4     2009                                    actew   55149.0  165307.0   \n",
       "...    ...                                      ...       ...       ...   \n",
       "5050  2021  digital australia investment management     149.0   59706.0   \n",
       "5051  2021                     mcdonald's australia     138.0   74021.0   \n",
       "5052  2021                  global switch australia      85.0  129503.0   \n",
       "5053  2021                   moorabool wind farm ()      64.0   68044.0   \n",
       "5054  2021              lyondellbasell australia ()       2.0   56327.0   \n",
       "\n",
       "      energy_consumption  total_emissions  \n",
       "0              1083428.0     1.221692e+10  \n",
       "1              1754221.0     1.201518e+09  \n",
       "2              1006315.0     1.661218e+10  \n",
       "3              1889997.0     8.943373e+09  \n",
       "4              2715397.0     5.514917e+10  \n",
       "...                  ...              ...  \n",
       "5050            249821.0     1.495971e+07  \n",
       "5051            331386.0     1.387402e+07  \n",
       "5052            576792.0     8.512950e+07  \n",
       "5053            250872.0     6.468044e+06  \n",
       "5054             58250.0     2.563270e+05  \n",
       "\n",
       "[5055 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"Keep only Firm Name, Scope 1 Emissions, Scope 2 Emissions, and Energy Consumption Columns\"\n",
    "nger_2012 = nger_2012.drop(nger_2012.columns[[3]],axis=1)\n",
    "nger_2013 = nger_2013.drop(nger_2013.columns[[4]],axis=1)\n",
    "nger_2014 = nger_2014.drop(nger_2014.columns[[1,5]],axis=1)\n",
    "nger_2015 = nger_2015.drop(nger_2015.columns[[1,5]],axis=1)\n",
    "nger_2016 = nger_2016.drop(nger_2016.columns[[1,5]],axis=1)\n",
    "nger_2017 = nger_2017.drop(nger_2017.columns[[1,5]],axis=1)\n",
    "nger_2018 = nger_2018.drop(nger_2018.columns[[1,5]],axis=1)\n",
    "nger_2019 = nger_2019.drop(nger_2019.columns[[1,5]],axis=1)\n",
    "nger_2020 = nger_2020.drop(nger_2020.columns[[1,5]],axis=1)\n",
    "nger_2021 = nger_2021.drop(nger_2021.columns[[1,5]],axis=1)\n",
    "\n",
    "\n",
    "\"Standarize Headings\"\n",
    "\n",
    "nger_list = [nger_2009,nger_2010,nger_2011,nger_2012,nger_2013,nger_2014,nger_2015,nger_2016,nger_2017,nger_2018,nger_2019,nger_2020,nger_2021]\n",
    "\n",
    "def standard_names(dataset):\n",
    "    \n",
    "    dataset.columns = ['firm_name', 'scope1', 'scope2','energy_consumption']\n",
    "\n",
    "for i in nger_list:\n",
    "    standard_names(i)\n",
    "\n",
    "\n",
    "\"Add Year To Datasets\"\n",
    "#add in year\n",
    "def add_year(dataset,year):\n",
    "    dataset.insert(0, 'year', year)\n",
    "#Registered Corporation Datasets\n",
    "add_year(nger_2009,2009)\n",
    "add_year(nger_2010,2010)\n",
    "add_year(nger_2011,2011)\n",
    "add_year(nger_2012,2012)\n",
    "add_year(nger_2013,2013)\n",
    "add_year(nger_2014,2014)\n",
    "add_year(nger_2015,2015)\n",
    "add_year(nger_2016,2016)\n",
    "add_year(nger_2017,2017)\n",
    "add_year(nger_2018,2018)\n",
    "add_year(nger_2019,2019)\n",
    "add_year(nger_2020,2020)\n",
    "add_year(nger_2021,2021)\n",
    "\n",
    "\n",
    "\"Construct Total Emissions\"\n",
    "\n",
    "def calculate_total_emissions(dataset):\n",
    "    dataset['total_emissions'] = dataset['scope1'] + dataset['scope2']\n",
    "    \n",
    "\n",
    "nger_list = [nger_2009,nger_2010,nger_2011,nger_2012,nger_2013,nger_2014,nger_2015,nger_2016,nger_2017,nger_2018,nger_2019,nger_2020,nger_2021]\n",
    "\n",
    "for i in nger_list:\n",
    "    calculate_total_emissions(i)\n",
    "\n",
    "\n",
    "\"Combined Datasets\"\n",
    "nger_data = pd.DataFrame(columns=['year', 'corporation', 'scope1', 'scope2', 'energy_consumption','total_emissions'])\n",
    "nger_list = [nger_2009,nger_2010,nger_2011,nger_2012,nger_2013,nger_2014,nger_2015,nger_2016,nger_2017,nger_2018,nger_2019,nger_2020,nger_2021]\n",
    "nger_data = pd.concat(nger_list)\n",
    "print(list(nger_data))\n",
    "\n",
    "\n",
    "\n",
    "\"Clean up nger_data\"\n",
    "\n",
    "#remove comma's from emissions values \n",
    "nger_data['scope1'] = nger_data['scope1'].replace(\",\", \"\", regex=True)\n",
    "nger_data['scope2'] = nger_data['scope2'].replace(\",\", \"\", regex=True)\n",
    "nger_data['total_emissions'] = nger_data['total_emissions'].replace(\",\", \"\", regex=True)\n",
    "nger_data['energy_consumption'] = nger_data['energy_consumption'].replace(\",\", \"\", regex=True)\n",
    "\n",
    "#remove spaces's from emissions values \n",
    "nger_data['scope1'] = nger_data['scope1'].replace(\" \", \"\", regex=True)\n",
    "nger_data['scope2'] = nger_data['scope2'].replace(\" \", \"\", regex=True)\n",
    "nger_data['total_emissions'] = nger_data['total_emissions'].replace(\" \", \"\", regex=True)\n",
    "nger_data['energy_consumption'] = nger_data['energy_consumption'].replace(\" \", \"\", regex=True)\n",
    "\n",
    "#remove alphanumeric charaters from emissions values\n",
    "nger_data['scope1'] = nger_data['scope1'].str.replace('[a-z]', '', regex=True, flags=re.IGNORECASE)\n",
    "nger_data['scope2'] = nger_data['scope2'].str.replace('[a-z]', '',regex=True, flags=re.IGNORECASE)\n",
    "nger_data['total_emissions'] = nger_data['total_emissions'].str.replace('[a-z]', '',regex=True, flags=re.IGNORECASE)\n",
    "nger_data['energy_consumption'] = nger_data['energy_consumption'].str.replace('[a-z]', '',regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "#replace blanks with nan\n",
    "nger_data = nger_data.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "#drop rows with missing scope1 & 2 data\n",
    "nger_data = nger_data.dropna(axis=0, how= 'all', subset=['scope1', 'scope2'])\n",
    "\n",
    "#convert emissions values to float\n",
    "nger_data['scope1'] = nger_data['scope1'].astype(float)\n",
    "nger_data['scope2'] = nger_data['scope2'].astype(float)\n",
    "nger_data['total_emissions'] = nger_data['total_emissions'].astype(float)\n",
    "nger_data['energy_consumption'] = nger_data['energy_consumption'].astype(float)\n",
    "\n",
    "#convert year column to date time format\n",
    "#nger_data['year'] =  pd.to_datetime(nger_data['year'], format='%Y').dt.to_period(\"Y\")\n",
    "nger_data['year'] = nger_data['year'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "\"FORMAT NGER COMPANY NAMES FOR MATCHING\"\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.lower()\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('pty ltd','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('limited group','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('limited','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('group','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('ltd','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('holdings','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('holding','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('pty','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('proprietary','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.replace('corporation','',regex=True)\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.rstrip('.')\n",
    "nger_data['firm_name']=nger_data['firm_name'].str.strip()\n",
    "nger_data = nger_data.rename(columns={'firm_name':'nger_name'})\n",
    "\n",
    "nger_data = nger_data.reset_index(drop=True)\n",
    "\n",
    "\"Extract unique nger firms\"\n",
    "nger_firms_list = []\n",
    "nger_firms_list = nger_data['nger_name'].unique().tolist()\n",
    "\n",
    "\n",
    "nger_data = nger_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\"Save to csv file\"\n",
    "output_filename = 'nger_data.csv'\n",
    "outputname = output_path + output_filename\n",
    "nger_data.to_csv(outputname, mode='w')\n",
    "print ('Exported: '+outputname)\n",
    "\n",
    "print('Number of Observations:')\n",
    "print (len(nger_data))\n",
    "\n",
    "\n",
    "with pd.option_context('display.max_rows', 100, 'display.max_columns', 100):\n",
    "    display(nger_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130502ce",
   "metadata": {},
   "source": [
    "# Clean Data: Morningstar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea2169e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported File: C:/Users/conno/OneDrive/University Study/Honours Thesis/cnolan-thesis./output/ms_data.csv\n",
      "Number of Observations:\n",
      "51819\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>morningstar_name</th>\n",
       "      <th>year</th>\n",
       "      <th>capex</th>\n",
       "      <th>eoy_price</th>\n",
       "      <th>eps</th>\n",
       "      <th>ltdebt</th>\n",
       "      <th>marketcap</th>\n",
       "      <th>ppe</th>\n",
       "      <th>revenue</th>\n",
       "      <th>roe</th>\n",
       "      <th>stdebt</th>\n",
       "      <th>assets</th>\n",
       "      <th>liabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14D</td>\n",
       "      <td>1414 degrees</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1AD</td>\n",
       "      <td>adalta</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1AE</td>\n",
       "      <td>aurora energy metals</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1AG</td>\n",
       "      <td>alterra</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1AL</td>\n",
       "      <td>oneall international</td>\n",
       "      <td>2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51814</th>\n",
       "      <td>LKE</td>\n",
       "      <td>lake resources nl</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51815</th>\n",
       "      <td>LKO</td>\n",
       "      <td>lakes blue energy nl</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51816</th>\n",
       "      <td>LKY</td>\n",
       "      <td>locksley resources</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51817</th>\n",
       "      <td>LLA</td>\n",
       "      <td>living and leisure australia</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51818</th>\n",
       "      <td>LLC</td>\n",
       "      <td>lendlease</td>\n",
       "      <td>2022</td>\n",
       "      <td>-85000000.0</td>\n",
       "      <td>9.11</td>\n",
       "      <td>-10.83</td>\n",
       "      <td>4.345000e+09</td>\n",
       "      <td>6.276790e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.710100e+10</td>\n",
       "      <td>1.013100e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51819 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker              morningstar_name  year       capex  eoy_price  \\\n",
       "0        14D                  1414 degrees  2000         NaN        NaN   \n",
       "1        1AD                        adalta  2000         NaN        NaN   \n",
       "2        1AE          aurora energy metals  2000         NaN        NaN   \n",
       "3        1AG                       alterra  2000         NaN        NaN   \n",
       "4        1AL          oneall international  2000         NaN        NaN   \n",
       "...      ...                           ...   ...         ...        ...   \n",
       "51814    LKE             lake resources nl  2022         NaN        NaN   \n",
       "51815    LKO          lakes blue energy nl  2022         NaN        NaN   \n",
       "51816    LKY            locksley resources  2022         NaN        NaN   \n",
       "51817    LLA  living and leisure australia  2022         NaN        NaN   \n",
       "51818    LLC                     lendlease  2022 -85000000.0       9.11   \n",
       "\n",
       "         eps        ltdebt     marketcap  ppe  revenue    roe  stdebt  \\\n",
       "0        NaN           NaN           NaN  NaN      NaN    NaN     NaN   \n",
       "1        NaN           NaN           NaN  NaN      NaN    NaN     NaN   \n",
       "2        NaN           NaN           NaN  NaN      NaN    NaN     NaN   \n",
       "3        NaN           NaN           NaN  NaN      NaN    NaN     NaN   \n",
       "4        NaN           NaN           NaN  NaN      NaN    NaN     NaN   \n",
       "...      ...           ...           ...  ...      ...    ...     ...   \n",
       "51814    NaN           NaN           NaN  NaN      NaN    NaN     NaN   \n",
       "51815    NaN           NaN           NaN  NaN      NaN    NaN     NaN   \n",
       "51816    NaN           NaN           NaN  NaN      NaN    NaN     NaN   \n",
       "51817    NaN           NaN           NaN  NaN      NaN    NaN     NaN   \n",
       "51818 -10.83  4.345000e+09  6.276790e+09  NaN      NaN  0.012     NaN   \n",
       "\n",
       "             assets   liabilities  \n",
       "0               NaN           NaN  \n",
       "1               NaN           NaN  \n",
       "2               NaN           NaN  \n",
       "3               NaN           NaN  \n",
       "4               NaN           NaN  \n",
       "...             ...           ...  \n",
       "51814           NaN           NaN  \n",
       "51815           NaN           NaN  \n",
       "51816           NaN           NaN  \n",
       "51817           NaN           NaN  \n",
       "51818  1.710100e+10  1.013100e+10  \n",
       "\n",
       "[51819 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"Melt Data to Time Series Format\"\n",
    "ms_capex = pd.melt(ms_capex, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='capex',col_level=None)\n",
    "ms_eoy_price = pd.melt(ms_eoy_price, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='eoy_price',col_level=None)\n",
    "ms_eps = pd.melt(ms_eps, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='eps',col_level=None)\n",
    "ms_ltdebt = pd.melt(ms_ltdebt, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='ltdebt',col_level=None)\n",
    "ms_marketcap = pd.melt(ms_marketcap, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='marketcap',col_level=None)\n",
    "ms_ppe = pd.melt(ms_ppe, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='ppe',col_level=None)\n",
    "ms_revenue = pd.melt(ms_revenue, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='revenue',col_level=None)\n",
    "ms_roe = pd.melt(ms_roe, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='roe',col_level=None)\n",
    "ms_stdebt = pd.melt(ms_stdebt, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='stdebt',col_level=None)\n",
    "ms_assets = pd.melt(ms_assets, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='assets',col_level=None)\n",
    "ms_liabilities = pd.melt(ms_liabilities, id_vars = ['ASX Code', 'Company Name', 'Item'], value_vars= ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022'], var_name = 'year', value_name='liabilities',col_level=None)\n",
    "\n",
    "\"Merge Data Sets\"\n",
    "\n",
    "#make capex the main dataset to merge on\n",
    "ms_data = ms_capex\n",
    "#merge eoy_price\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex']], ms_eoy_price[['ASX Code', 'year','eoy_price']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge eps\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price']], ms_eps[['ASX Code', 'year','eps']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge ltdebt\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price','eps']], ms_ltdebt[['ASX Code', 'year','ltdebt']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge marketcap\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price','eps','ltdebt']], ms_marketcap[['ASX Code', 'year','marketcap']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge ppe\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price','eps','ltdebt','marketcap']],  ms_ppe[['ASX Code', 'year','ppe']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge revenue\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price','eps','ltdebt','marketcap','ppe']], ms_revenue[['ASX Code', 'year','revenue']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge roe\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price','eps','ltdebt','marketcap','ppe','revenue']], ms_roe[['ASX Code', 'year','roe']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge stdebt\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price','eps','ltdebt','marketcap','ppe','revenue','roe']], ms_stdebt[['ASX Code', 'year','stdebt']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge assets\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price','eps','ltdebt','marketcap','ppe','revenue','roe','stdebt']], ms_assets[['ASX Code', 'year','assets']],on = ['ASX Code', 'year'], how = 'inner')\n",
    "#merge liabilities\n",
    "ms_data = pd.merge(ms_data[['ASX Code', 'Company Name','year', 'capex','eoy_price','eps','ltdebt','marketcap','ppe','revenue','roe','stdebt','assets']], ms_liabilities[['ASX Code', 'year','liabilities']], on = ['ASX Code', 'year'], how = 'inner')\n",
    "\n",
    "\n",
    "\"Clean up ['capex']\"\n",
    "#remove comma's from capex column\n",
    "ms_data[\"capex\"] = ms_data[\"capex\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"capex\"] = ms_data[\"capex\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"capex\"] = ms_data[\"capex\"].replace('--', np.nan, regex=True)\n",
    "#convert capex values to float\n",
    "ms_data['capex'] = ms_data['capex'].astype(float)\n",
    "\n",
    "\"Clean up ['eoy_price']\"\n",
    "#remove comma's from eoy_price values \n",
    "ms_data[\"eoy_price\"] = ms_data[\"eoy_price\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"eoy_price\"] = ms_data[\"eoy_price\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"eoy_price\"] = ms_data[\"eoy_price\"].replace('--', np.nan, regex=True)\n",
    "#convert eoy_price values to float\n",
    "ms_data['eoy_price'] =ms_data['eoy_price'].astype(float)\n",
    "#replace negative numbers with nan\n",
    "ms_data.loc[ms_data['eoy_price'] < 0, 'eoy_price'] = np.nan\n",
    "#replace 0 with nan\n",
    "ms_data['eoy_price'] = ms_data['eoy_price'].replace(0, np.nan)\n",
    "\n",
    "\n",
    "\"Clean up ['eps']\"\n",
    "#remove comma's from revenue values \n",
    "ms_data[\"eps\"] = ms_data[\"eps\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"eps\"] = ms_data[\"eps\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"eps\"] = ms_data[\"eps\"].replace('--', np.nan, regex=True)\n",
    "#convert eps values to float\n",
    "ms_data['eps'] = ms_data[\"eps\"].astype(float)\n",
    "\n",
    "\"Clean up ['ltdebt']\"\n",
    "#remove comma's from ltdebt  \n",
    "ms_data[\"ltdebt\"] = ms_data[\"ltdebt\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"ltdebt\"] = ms_data[\"ltdebt\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"ltdebt\"] = ms_data[\"ltdebt\"].replace('--', np.nan, regex=True)\n",
    "#convert ltdebt values to float\n",
    "ms_data['ltdebt'] = ms_data['ltdebt'].astype(float)\n",
    "#replace 0 with nan\n",
    "ms_data['ltdebt'] = ms_data['ltdebt'].replace(0, np.nan)\n",
    "#replace negative numbers with nan\n",
    "ms_data.loc[ms_data['ltdebt'] < 0, 'ltdebt'] = np.nan\n",
    "\n",
    "\"Clean up ['marketcap']\"\n",
    "#replace blanks with nan\n",
    "ms_data['marketcap'] = ms_data['marketcap'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#convert marketcap values to float\n",
    "ms_data['marketcap'] = ms_data['marketcap'].astype(float)\n",
    "#replace negative numbers with nan\n",
    "ms_data.loc[ms_data['marketcap'] < 0, 'marketcap'] = np.nan\n",
    "#replace 0 with nan\n",
    "ms_data['marketcap'] = ms_data['marketcap'].replace(0, np.nan)\n",
    "\n",
    "\"Clean up ['ppe']\"\n",
    "\n",
    "#remove comma's from ppe value \n",
    "ms_data[\"ppe\"] = ms_data[\"ppe\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"ppe\"] = ms_data[\"ppe\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"ppe\"] = ms_data[\"ppe\"].replace('--', np.nan, regex=True)\n",
    "#convert ppe values to float\n",
    "ms_data['ppe'] = ms_data[\"ppe\"].astype(float)\n",
    "#replace negative numbers with nan\n",
    "ms_data.loc[ms_data['ppe'] < 0, 'ppe'] = np.nan\n",
    "\n",
    "\"Clean up ['revenue']\"\n",
    "#remove comma's from revenue values \n",
    "ms_data[\"revenue\"] = ms_data[\"revenue\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"revenue\"] = ms_data[\"revenue\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"revenue\"] = ms_data[\"revenue\"].replace('--', np.nan, regex=True)\n",
    "#convert revenue values to float\n",
    "ms_data['revenue'] = ms_data[\"revenue\"].astype(float)\n",
    "#replace negative numbers with nan\n",
    "ms_data.loc[ms_data['revenue'] < 0, 'revenue'] = np.nan\n",
    "#replace 0 with nan\n",
    "ms_data['revenue'] = ms_data['revenue'].replace(0, np.nan)\n",
    "\n",
    "\"Clean up ['roe']\"\n",
    "#remove comma's from roe \n",
    "ms_data[\"roe\"] = ms_data[\"roe\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"roe\"] = ms_data[\"roe\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"roe\"] = ms_data[\"roe\"].replace('--', np.nan, regex=True)\n",
    "#convert percentage string to float for roe \n",
    "ms_data['roe'] = (pd.to_numeric(ms_data['roe'].str[:-1]).div(100).mask(ms_data['roe']=='%',0))\n",
    "\n",
    "\"Clean up ['stdebt']\"\n",
    "#remove comma's from stdebt  \n",
    "ms_data[\"stdebt\"] = ms_data[\"stdebt\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"stdebt\"] = ms_data[\"stdebt\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"stdebt\"] = ms_data[\"stdebt\"].replace('--', np.nan, regex=True)\n",
    "#convert stdebt values to float\n",
    "ms_data['stdebt'] = ms_data[\"stdebt\"].astype(float)\n",
    "#replace negative numbers with nan\n",
    "ms_data.loc[ms_data['stdebt'] < 0, 'stdebt'] = np.nan\n",
    "#replace 0 with nan\n",
    "ms_data['stdebt'] = ms_data['stdebt'].replace(0, np.nan)\n",
    "\n",
    "\"Clean up ['assets']\"\n",
    "#remove comma's from total asset value \n",
    "ms_data[\"assets\"] = ms_data[\"assets\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"assets\"] = ms_data[\"assets\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"assets\"] = ms_data[\"assets\"].replace('--', np.nan, regex=True)\n",
    "#convert marketcap values to float\n",
    "ms_data[\"assets\"] = ms_data[\"assets\"].astype(float)\n",
    "#replace negative numbers with nan\n",
    "ms_data.loc[ms_data['assets'] < 0, 'assets'] = np.nan\n",
    "#replace 0 with nan\n",
    "ms_data['assets'] = ms_data['assets'].replace(0, np.nan)\n",
    "\n",
    "\"Clean up ['liabilities']\"\n",
    "#remove comma's from total asset value \n",
    "ms_data[\"liabilities\"] = ms_data[\"liabilities\"].replace(\",\", \"\", regex=True)\n",
    "#replace blanks with nan\n",
    "ms_data[\"liabilities\"] = ms_data[\"liabilities\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#replace '--'' with nan\n",
    "ms_data[\"liabilities\"] = ms_data[\"liabilities\"].replace('--', np.nan, regex=True)\n",
    "#convert marketcap values to float\n",
    "ms_data['liabilities'] = ms_data[\"liabilities\"].astype(float)\n",
    "#replace negative numbers with nan\n",
    "ms_data.loc[ms_data['liabilities'] < 0, 'liabilities'] = np.nan\n",
    "\"Convert year column to date time format\"\n",
    "ms_data['year'] =  pd.to_datetime(ms_data['year'], format='%Y').dt.to_period(\"Y\")\n",
    "\n",
    "\"FORMAT MORNINGSTAR COMPANY NAMES FOR MATCHING\"\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.lower()\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('pty ltd','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('limited group','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('limited','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('group','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('ltd.','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('ltd','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('holdings','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('holding','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('pty.','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('proprietary','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.replace('corporation','',regex=True)\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.rstrip('.')\n",
    "ms_data['Company Name']=ms_data['Company Name'].str.strip()\n",
    "ms_data = ms_data.rename(columns={'Company Name':'morningstar_name','ASX Code':'ticker'})\n",
    "\n",
    "\n",
    "\"Save collated file\"\n",
    "output_filename = 'ms_data.csv'\n",
    "outputname = output_path + output_filename\n",
    "ms_data.to_csv(outputname, mode='w', index=False)\n",
    "print(\"Exported File: \" + outputname)\n",
    "\n",
    "print('Number of Observations:')\n",
    "print (len(ms_data))\n",
    "\n",
    "\n",
    "with pd.option_context('display.max_rows', 100, 'display.max_columns', None):\n",
    "    display(ms_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "157f5286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook Finish\n"
     ]
    }
   ],
   "source": [
    "print('Notebook Finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
